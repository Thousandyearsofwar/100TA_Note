# 基础夯实

+ ## 数学基础

  + <font size=4>**1.向量运算**</font>
  + <font size=4>**2.矩阵运算**</font>
  + <font size=4>**3.MVP矩阵推导**</font>
  + <font size=4>**4.傅里叶变换**</font>
  + <font size=4>**5.其他**</font>

+ ## 纹理介绍

  + <font size=4>**纹理三大问**</font>
    + 纹理是什么：一种可供着色器读写的结构化存储形式
     Image[Height][Width][4]
    + 为什么出现了纹理：
      + 通过牺牲几何细节，减低了建模工作量，减低了模型数据储存空间，同时也提升了读取速度。能够高效修改表面材质和模型。[Texturing is a technique for efficiently modeling variations in a surface’s material and
finish.]
    + 纹理管线：
      模型空间位置=>投影函数[展uv阶段用到的:平面映射，圆柱映射，立方体映射]
      =>纹理映射=>纹理坐标=>通讯函数[Offset,Tilling]=>新纹理坐标=>
      纹理采样[避免依赖纹理读取][SAMPLER:Uniform类型的变量]=>纹理值
      + 依赖纹理读取：在RTR4中的原文解释是[^1]
  + <font size=4>**纹素**</font>  
    + 纹素：图像纹理上的像素通常被称为纹素，以区分它和屏幕上的像素。[^2]
  + <font size=4>**纹理管线 Texturing pipeline**</font> 
    + 纹理管线：纹理化的过程广义上被描述成为纹理化管线。[^3]
    + 纹理管线的过程:纹理化的始点一般是模型上的局部坐标，也可以是世界坐标。以达到模型移动之后，纹理也跟着移动的目的。使用投影函数(projector function)，获取到的坐标叫做纹理坐标(texture coordinates)，投影函数被用作于获取纹理。这一过程叫做映射(Mapping)。有的时候用作纹理的图像就被叫做为纹理，这其实是不太严谨的。
      在使用刚刚获取的纹理坐标来获取纹理之前，还会使用一个或多个通讯函数(corresponder functions)来将纹理坐标转换到纹理空间(Texture Space location)。纹理空间的坐标将会从纹理获取对应的值(Obtain value)，而获取到的值还可以是数组的索引值来检索另外一张图像纹理的像素，会被进一步被潜在地被转换(value transform function)，最后获取到的数值被用作于修改模型表面的某种属性。[^4]
      + 例子：一面砖墙的例子
      模型空间坐标(x,y,z)为(-2.3,7.1,88.2)---Object space location
      使用投影函数[projector function]将三维坐标(x,y,z)转换成二维坐标(u,v)
      假设投影得到的UV坐标为(0.32,0.29)，这个纹理坐标将会用于查找图像上的颜色---parameter space coordinates
      假设纹理分辨率为256*256，所以在通信函数中将返回实际图像中坐标位置(0.32,0.29) * (256,256)=(81.92,74.24)。---texture space location
      去掉分数后，像素(81,74)就在图像上会被寻获，得到颜色值(0.9,0.8,0.7)。---texture value
      由于纹理的颜色空间是SRGB，想用于着色计算就必须转换成Linear空间，最后转换为(0.787,0.604,0.448)---transformed texture value
      注：本例使用的投影方式相当于正交投影，砖墙其表面的一个值经过这个投影方式返回[0,1]的UV值

  + <font size=4>**优化与纹理应用**</font>



[^1]: One term worth explaining at this point is dependent texture read, which has two definitions. The first applies to mobile devices in particular.**When accessing a texture via texture2D or similar, a dependent texture read occurs whenever the pixel shader calculates texture coordinates instead of using the unmodified texture coordinates passed in from the vertex shader [66].** Note that this means any change at all to the incoming texture coordinates, even such simple actions as swapping the u and v values. Older mobile GPUs, those that do not support OpenGL ES 3.0, run more efficiently when the shader has no dependent texture reads, as the texel data can then be prefetched. **The other, older, definition of this term was particularly important for early desktop GPUs. In this context a dependent texture read occurs when one texture’s coordinates are dependent on the result of some previous texture’s values.** For example, one texture might change the shading normal, which in turn changes the coordinates used to access a cube map. Such functionality was limited or even non-existent on early GPUs. Today such reads can have an impact on performance, depending on the number of pixels being computed in a batch, among other factors.See Section 23.8 for more information.
    机翻：在这一点上值得解释的一个术语是依赖纹理读取，它有两个定义。 第一种特别适用于移动设备。 **当通过texture2D或类似的方法访问纹理时，每当像素着色器计算纹理坐标时，就会发生一个相关的纹理读取，而不是使用从顶点着色器[66]传入的未修改的纹理坐标。** 请注意，这意味着对传入纹理坐标的任何更改，即使是交换u和v值这样的简单操作。 旧的移动GPU，那些不支持OpenGLES3.0的，当着色器没有依赖的纹理读取时，运行效率更高，因为文本数据可以被预取。**对于早期的桌面GPU来说，这个术语的另一个更古老的定义尤为重要。 在此上下文中，当一个纹理的坐标依赖于某些先前纹理的值的结果时，就会发生依赖的纹理读取。** 例如，一个纹理可能会改变着色法线，从而又会改变用于访问立方体映射的坐标。这些功能在早期的GPU上是有限的，甚至是不存在的。今天，这种读取可以影响性能，这取决于在批处理中计算的像素数，以及其他因素。 详情见第23.8节。

[^2]: The pixels in the image texture are often called texels, to differentiate them from the pixels on the screen.

[^3]: Texturing can be described by a generalized texture pipeline.

[^4]: A location in space is the starting point for the texturing process. This location can be in **world space**, but is more often in the **model’s frame of reference**, so that as the model moves, the texture moves along with it. Using Kershaw’s terminology [884], this point in space then has a **projector function** applied to it to obtain a set of numbers, called **texture coordinates**, that will be used for accessing the texture. This process is called mapping, which leads to the phrase **texture mapping**. Sometimes the texture image itself is called the texture map, though this is not strictly correct.
Before these new values may be used to access the texture, one or more **corresponder functions** can be used to transform the texture coordinates to texture space. These texture-space locations are used to **obtain values** from the texture, e.g., they may be array indices into an image texture to retrieve a pixel. The retrieved values are then potentially transformed yet again by a **value transform function**, and finally these new values are used to modify some property of the surface, such as the material or shading normal. Figure 6.2 shows this process in detail for the application of a single texture. The reason for the complexity of the pipeline is that each step provides the user with a useful control. It should be noted that not all steps need to be activated at all times.